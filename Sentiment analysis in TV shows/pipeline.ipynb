{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_audio, ffmpeg_extract_subclip\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import whisper\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and defining label2id\n",
    "data_model = pd.read_csv(\"./data/5000_sampled.csv\")\n",
    "data_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at model/roberta_multi_model_v6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved ROBERTA model\n",
    "label2id = {label: i for i, label in enumerate(data_model['emotion'].unique())}\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('model/roberta_multi_model_v6', num_labels=len(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>Start_time</th>\n",
       "      <th>End_time</th>\n",
       "      <th>Emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Joy, Surprise, Anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>350</td>\n",
       "      <td>383.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>383</td>\n",
       "      <td>443.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>443</td>\n",
       "      <td>504.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Episode  Start_time  End_time                     Emotions\n",
       "0        1           0     124.0                      neutral\n",
       "1        1         124     350.0  Joy, Surprise, Anticipation\n",
       "2        1         350     383.0                      neutral\n",
       "3        1         383     443.0                      neutral\n",
       "4        1         443     504.0                      neutral"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = pd.read_csv(\"../speach_to_text/data_robinson/Robinson_filtered.csv\")\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_episodes_into_fragments(data_path=\"../speach_to_text/data_robinson/Robinson_filtered.csv\", source_path=\"../speach_to_text/episodes/full_episodes/*\", target_path=\"./data/episodes_cut/\"):\n",
    "    print(\"start of split_episodes_into_fragments\")\n",
    "    # Read the data\n",
    "    data_filtered = pd.read_csv(data_path)\n",
    "    \n",
    "    # Create a directory for temporary clips\n",
    "    os.makedirs(\"./data/temp_clips\", exist_ok=True)\n",
    "    \n",
    "    for file_path in glob.glob(source_path):\n",
    "        episode_number = int(os.path.basename(file_path)[8:10])\n",
    "        temp_df = data_filtered[data_filtered['Episode'] == episode_number]\n",
    "        temp_df = temp_df[['Start_time', 'End_time']]\n",
    "        for row_index in range(temp_df.shape[0]):\n",
    "            start = temp_df.iloc[row_index, temp_df.columns.get_loc('Start_time')]\n",
    "            end = temp_df.iloc[row_index, temp_df.columns.get_loc('End_time')]\n",
    "            clip_path = f\"data/temp_clips/ep_{episode_number}_{start}_{end}.mov\"\n",
    "            final_path = f\"data/episodes_cut/{os.path.basename(clip_path[:-4])}.mp3\"\n",
    "            ffmpeg_extract_subclip(file_path, start, end, clip_path)\n",
    "            ffmpeg_extract_audio(clip_path, final_path)\n",
    "    \n",
    "    # Remove temporary directory\n",
    "    shutil.rmtree(\"./data/temp_clips\")\n",
    "    print(\"end of split_episodes_into_fragments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the transcript from the fragments\n",
    "def speach_to_text(data_filtered):\n",
    "    print(\"start of speach_to_text\")\n",
    "    transcript = []\n",
    "\n",
    "    for row_index in range(len(data_filtered)):\n",
    "        episode_number = data_filtered.iloc[row_index, 0]\n",
    "        start = data_filtered.iloc[row_index, 1]\n",
    "        end = data_filtered.iloc[row_index, 2]\n",
    "\n",
    "        clip_path = f\"data/episodes_cut/ep_{episode_number}_{start}_{end}.mp3\"\n",
    "\n",
    "        # Load the Whisper model and transcribe the audio file\n",
    "        model = whisper.load_model(\"base\")\n",
    "        result = model.transcribe(clip_path, fp16=False, language=\"en\")\n",
    "        transcript.append(result[\"text\"])\n",
    "\n",
    "    print(\"end of speach_to_text\")\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcript(data_filtered, transcript, output_path='./data/transcript.csv'):\n",
    "    print(\"start of save_transcript\")\n",
    "    # Merge the transcript with data_filtered\n",
    "    transcript_df = pd.DataFrame({\n",
    "        'Episode Number': data_filtered['Episode'],\n",
    "        'Start': data_filtered['Start_time'],\n",
    "        'End': data_filtered['End_time'],\n",
    "        'Emotions': data_filtered['Emotions'],\n",
    "        'Transcript': transcript\n",
    "    })\n",
    "    \n",
    "    # Write the merged DataFrame to a CSV file\n",
    "    transcript_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"end of save_transcript\")\n",
    "    return transcript_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'anger': 0,\n",
    "    'disgust': 1,\n",
    "    'fear': 2,\n",
    "    'happiness': 3,\n",
    "    'sadness': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_emotions(model, data_filtered_transcript):\n",
    "    print(\"start of predict_emotions\")\n",
    "    # model.eval()\n",
    "\n",
    "    # Clean the text data\n",
    "    data_filtered_transcript = data_filtered_transcript.fillna(\" \")\n",
    "\n",
    "    # Tokenize the input texts for Kaggle test set\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    # inputs = tokenizer(data_filtered_transcript['Transcript'].tolist(), truncation=True, padding=True)\n",
    "    inputs = tokenizer(data_filtered_transcript['Transcript'].tolist(), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    \n",
    "    batch_size = 32\n",
    "    num_samples = len(inputs['input_ids'])\n",
    "\n",
    "    # Create an empty list to store the predictions\n",
    "    predictions = []\n",
    "\n",
    "    # Iterate over the input samples in batches\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # Get the batch inputs\n",
    "        batch_inputs = {key: value[i:i+batch_size] for key, value in inputs.items()}\n",
    "        \n",
    "        # Perform inference on the batch\n",
    "        batch_outputs = model(batch_inputs)\n",
    "        \n",
    "        # Get the batch predictions\n",
    "        batch_predictions = tf.argmax(batch_outputs.logits, axis=1)\n",
    "        \n",
    "        # Append the batch predictions to the list\n",
    "        predictions.extend(batch_predictions.numpy())\n",
    "\n",
    "    # Convert the predictions to a DataFrame column\n",
    "    predictions_df = pd.DataFrame({'prediction': predictions})\n",
    "\n",
    "    # Map the predictions to the original labels\n",
    "    predictions = predictions_df['prediction'].map({v: k for k, v in label_mapping.items()})\n",
    "\n",
    "    # # Assign the predictions back to the original DataFrame\n",
    "    # test_df['prediction'] = predictions_df['prediction']\n",
    "\n",
    "\n",
    "    # # Map the predictions to the original labels\n",
    "    # test_df['prediction'] = test_df['prediction'].map({v: k for k, v in label_mapping.items()})\n",
    "\n",
    "    # Create Dataframe for submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Episode Number': data_filtered_transcript['Episode Number'],\n",
    "        'Start': data_filtered_transcript['Start'],\n",
    "        'End': data_filtered_transcript['End'],\n",
    "        'Emotions': data_filtered_transcript['Emotions'],\n",
    "        'Transcript': data_filtered_transcript['Transcript'],\n",
    "        'Predicted Emotions': predictions\n",
    "    })\n",
    "    print(\"end of predict_emotions\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline function\n",
    "def pipeline(data_path=\"../speach_to_text/data_robinson/Robinson_filtered.csv\", source_path=\"../speach_to_text/episodes/full_episodes/*\", target_path=\"./data/episodes_cut/\", output_path='data/transcript.csv'):\n",
    "\n",
    "    print(\"start of pipeline\")\n",
    "    # Split episodes into fragments\n",
    "    if len(os.listdir('data/episodes_cut/')) == 0:\n",
    "        split_episodes_into_fragments()\n",
    "    \n",
    "   \n",
    "    if os.path.exists(output_path):\n",
    "        transcript_df = pd.read_csv(output_path)\n",
    "    else:\n",
    "        data_filtered = pd.read_csv(data_path)\n",
    "        transcript = speach_to_text(data_filtered) # Get transcript from fragments\n",
    "        transcript_df = save_transcript(data_filtered, transcript, output_path) # Save transcript\n",
    "    \n",
    "    # Predict emotions\n",
    "    submission_df = predict_emotions(model, transcript_df)\n",
    "    print(\"end of pipeline\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the pipeline function to execute the entire process\n",
    "result_df = pipeline()\n",
    "result_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
