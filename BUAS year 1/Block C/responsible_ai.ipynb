{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block C: Responsible AI \n",
    "\n",
    "For details regarding ILO 3.1 and the use-case, please refer to the Assessment rubric in Microsoft Teams, and the [DataLab: Responsible AI](https://adsai.buas.nl/Study%20Content/Responsible%20and%20Explainable%20AI/UseCases.html) GitHub page.\n",
    "\n",
    "## Use-case 1: Identifying, and describing bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Define the concept of bias in relation to the Imsitu dataset, and a computer vision task:**\n",
    "\n",
    "Bias in relation to the Imsitu dataset and a computer vision task refers to the tendency of a machine learning algorithm to make decisions that are not based on facts or data, but instead on the programmer's own preconceived notions, stereotypes, or preferences. This can lead to incorrect or unfair results for certain populations or groups, leading to a lack of fairness in the system. Bias in computer vision tasks can be caused by a variety of factors, including the dataset used to train the model, the type of algorithm chosen, and the programmer's own assumptions.\n",
    "\n",
    "**- List, and describe the type of bias that you identified in the Imsitu dataset:**\n",
    "\n",
    "Representation Bias:\n",
    "When we look at praying there are more images of people praying with both hands put together than not. Out of the 19 pictures that show up when searching for the verb praying, only 2 are pictures of hands not put together.\n",
    "\n",
    "**- Discuss the possible ramification (e.g., harm) in terms of fairness of the identified bias instance:\n",
    "Why, and when, is this particular instance of bias undesirable? In other words, who might be disproportionally affected by this particular instance of bias, and when does this negative effect come into play?**\n",
    "\n",
    "This negatively affects the different religions. Most of the pictures are Christian people praying, but this disproportionately affects other religions such as Hinduism, Islam, Judaism, Buddhism, etc. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 2: Propose individual fairness method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Identify a sensitive/protected attribute in the Imsitu dataset:**\n",
    "Race/ Ethnicity\n",
    "\n",
    "**- Mitigate bias in the Imsitu dataset by applying the ‘Fairness Through Unawareness' or ‘Fairness Through Awareness' method to this sensitive/protected attribute.**\n",
    "\n",
    "Fairness through Unawareness: This would remove all the incorrect labels that are related to Race or ethnicity form the dataset. This would make sure that the dataset does not contain any information about the race/ethnicity of the people in the images, and  any potential biases would be eliminated.\n",
    "\n",
    "**- Elaborate on the individual fairness method that you applied, and why you think it is a good method to mitigate bias in the Imsitu dataset:**\n",
    "\n",
    "This method is a good to medicate bias because it eliminates all the incorrect labels. This allows the dataset to remain unbiased from any potential discrimination or prejudice. The method also allows the dataset to remain open and accessible to all users, regardless of their race/ethnicity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 3: Create a subset of images from the original dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your text for use-case 3 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your Python code for use-case 3 here\n",
    "#Determine which agent codes are associated with which nouns:\n",
    "import json\n",
    "\n",
    "# load imsitu_space.json file\n",
    "imsitu_space = json.load(open(\"Data/imsitu_space.json\"))\n",
    "\n",
    "nouns = imsitu_space[\"nouns\"]\n",
    "verbs = imsitu_space[\"verbs\"]\n",
    "\n",
    "# function to get all agent codes for a specific agent/noun\n",
    "def get_agent_codes(agent = \"banana\"):\n",
    "    for noun in nouns:\n",
    "        if nouns[noun]['gloss'][0] in agent:\n",
    "            print(f\"{agent} found\")\n",
    "            print(noun)\n",
    "\n",
    "# get all agent codes for men (use your own nouns here)\n",
    "get_agent_codes(\"banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to extract the images that contain a specific verb and agent value:\n",
    "import json\n",
    "\n",
    "def get_verb_agent(json_file, verb_custom, agent_custom):\n",
    "    train = json.load(open(json_file))\n",
    "    verb_value = []\n",
    "    agent_key = []\n",
    "    agent_value = []\n",
    "    file_path = []\n",
    "    count = 0\n",
    "    for i in train:\n",
    "        verb = train[i]['verb']\n",
    "        if verb == verb_custom:\n",
    "            frames = train[i]['frames']\n",
    "            for frame in frames:\n",
    "                for key, value in frame.items():\n",
    "                    if key == 'agent':\n",
    "                        if value in agent_custom:\n",
    "                            if i not in file_path:\n",
    "                                agent_key.append(key)\n",
    "                                agent_value.append(value)\n",
    "                                file_path.append(i)\n",
    "                                verb_value.append(verb)\n",
    "                                count += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "    return(file_path, verb_value, agent_key, agent_value, count)\n",
    "\n",
    "get_verb_agent('Data/train.json', 'spoiling', ['n12352287', 'n07753592' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the images that contain the predefined verb and agent values, and storing them in a new folder\n",
    "import shutil\n",
    "\n",
    "def img_to_folder(dirs_original, dirs_destination):\n",
    "    image_list = get_verb_agent('Data/train.json', 'dusting', ['n12352287', 'n07753592' ])[0]\n",
    "    dirs_list = [(dirs_original, dirs_destination)]\n",
    "    for img in image_list:\n",
    "        for source_folder, destination_folder in dirs_list:\n",
    "            shutil.copy(source_folder+img, destination_folder+img)\n",
    "\n",
    "img_to_folder(\"Data/dev.json\", \"Data/Spoiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a csv file that contains the file name, verb and agent value for each image\n",
    "import pandas as pd\n",
    "\n",
    "def lists_to_df(dirs_destination, col1_name, col2_name, col3_name):\n",
    "    col1 = get_verb_agent('Data/train.json', 'spoiling', ['n12352287', 'n07753592' ])[0]\n",
    "    col2 = get_verb_agent('Data/train.json', 'spoiling', ['n12352287', 'n07753592' ])[1]\n",
    "    col3 = get_verb_agent('Data/train.json', 'spoiling', ['n12352287', 'n07753592' ])[3]\n",
    "    df = pd.DataFrame(list(zip(col1, col2, col3)), columns=[col1_name, col2_name, col3_name])\n",
    "    df.to_csv(dirs_destination, index=False)\n",
    "    return df\n",
    "\n",
    "lists_to_df('Data/Dusting_train.csv', 'file_name','spoiling', 'banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imsitu= pd.read_csv(r\"C:/Users/neilr/github-classroom/BredaUniversityADSAI/2022-23c-1fcmgt-reg-ai-01-neildaniel221270/Data/imsitu2.csv\")\n",
    "# create a new column 'age_group' based on the values in the 'agent' column\n",
    "df_imsitu['condition'] = df_imsitu['agent'].apply(lambda x: 'spoiled' if x in ['n12352287', 'n07753592'] else 'yellow')\n",
    "df_imsitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the path to the image directory of google scraped images using a raw string\n",
    "image_dir = r\"C:/Users/neilr/OneDrive/Documents/BUAS/Year 1/Block C/Creative brief/Data/Resized bannanas/spoiled\"\n",
    "\n",
    "# Get a list of all image file names in the directory\n",
    "image_files = os.listdir(image_dir)\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data = {\"file_name\": image_files, \"condition\": [\"spoiled\"] * len(image_files)}\n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df_spoilscraped = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df_spoilscraped.to_csv(\"Data/image_spoiled_scraped.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_igspoil = pd.merge(df_imsitu, df_spoilscraped, on=['file_name', 'condition'], how='outer')\n",
    "df_igspoil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the image directory of google scraped images using a raw string\n",
    "image_dir = r\"C:/Users/neilr/OneDrive/Documents/BUAS/Year 1/Block C/Creative brief/Data/Resized bannanas/not_spoiled\"\n",
    "\n",
    "# Get a list of all image file names in the directory\n",
    "image_files = os.listdir(image_dir)\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data = {\"file_name\": image_files, \"condition\": [\"not spoiled\"] * len(image_files)}\n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df_notspoiled = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df_notspoiled.to_csv(\"Data/image_notspoiled_scraped.csv\", index=False)\n",
    "df_notspoiled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_igbananas = pd.merge(df_igspoil, df_notspoiled, on=['file_name', 'condition'], how='outer')\n",
    "df_igbananas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X represents the feature matrix, and y represents the labels\n",
    "train_df, test_df = train_test_split(df_igbananas, test_size=0.2, stratify=df_igbananas[\"condition\"], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the size of each set\n",
    "print(f'Training set size: {len(train_df)}')\n",
    "print(f'Validation set size: {len(val_df)}')\n",
    "print(f'Testing set size: {len(test_df)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 4: Write Python functions; group fairness metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your text for use-case 4 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your Python code for use-case 4 here\n",
    "\n",
    "def load_confusion_matrices():\n",
    "    cm_priv = np.load('./Responsible_AI/confusion_matrix_priv_female.npy')\n",
    "    tn_priv = cm_priv[0][0]\n",
    "    fp_priv = cm_priv[0][1]\n",
    "    fn_priv = cm_priv[1][0]\n",
    "    tp_priv = cm_priv[1][1]\n",
    "    cm_unpriv = np.load('./Responsible_AI/confusion_matrix_unpriv_male.npy')\n",
    "    tn_unpriv = cm_unpriv[0][0]\n",
    "    fp_unpriv = cm_unpriv[0][1]\n",
    "    fn_unpriv = cm_unpriv[1][0]\n",
    "    tp_unpriv = cm_unpriv[1][1]\n",
    "    return [[tn_priv, fp_priv, fn_priv, tp_priv], [tn_unpriv, fp_unpriv, fn_unpriv, tp_unpriv]]\n",
    "\n",
    "load_confusion_matrices()\n",
    "\n",
    "def demographic_parity():\n",
    "    [[tn,fp,fn,tp],[tnm,fpm,fnm,tpm]]=load_confusion_matrices()\n",
    "    NR = (tp + fp)/(tn + fp + fn + tp)\n",
    "    NRm = (tpm + fpm)/(tnm + fpm + fnm + tpm)\n",
    "    return [NR, NRm, NR-NRm]\n",
    "demographic_parity()\n",
    "\n",
    "def predictive_parity():\n",
    "    [[tn,fp,fn,tp],[tnm,fpm,fnm,tpm]]=load_confusion_matrices()\n",
    "    PPV = tp/(tp+fp)\n",
    "    PPVm = tpm/(tpm+fpm)\n",
    "    return [PPVm, PPV, PPVm - PPV]\n",
    "predictive_parity()\n",
    "\n",
    "def equalized_odds():\n",
    "    [[tn,fp,fn,tp],[tnm,fpm,fnm,tpm]]=load_confusion_matrices()\n",
    "    EO = (tp )/(tp + fn )\n",
    "    EOm = (tpm)/(tpm + fnm )\n",
    "    EOf = (tn )/(tn + fp )\n",
    "    EOmf = (tnm)/(tnm + fpm )\n",
    "    return [EO - EOm, EOmf-EOf]\n",
    "equalized_odds()\n",
    "\n",
    "def conditional_use_accuracy_equality():\n",
    "    [[tn,fp,fn,tp],[tnm,fpm,fnm,tpm]]=load_confusion_matrices()\n",
    "    CAE_PPV = - (tp )/(tp + fp ) + (tpm )/(tpm + fpm )\n",
    "    CAE_NPV = - (tn)/(tn + fn) + (tnm)/(tnm + fnm)\n",
    "    return [CAE_PPV, CAE_NPV]\n",
    "conditional_use_accuracy_equality()\n",
    "\n",
    "def equal_selection_parity():\n",
    "    [[tn,fp,fn,tp],[tnm,fpm,fnm,tpm]]=load_confusion_matrices()\n",
    "    SP = tp +fp\n",
    "    SPm = tpm + fpm\n",
    "    return [SPm,SP, SP - SPm]\n",
    "equal_selection_parity()\n",
    "\n",
    "def equal_opportunity():\n",
    "    [[tn,fp,fn,tp],[tnm,fpm,fnm,tpm]]=load_confusion_matrices()\n",
    "    EOm = tp/(fn+tp)\n",
    "    EO = tpm/(fnm+tpm)\n",
    "    return [EO, EOm, EOm - EO]\n",
    "equal_opportunity()\n",
    "\n",
    "def predictive_equality():\n",
    "    [[tn,fp,fn,tp],[tnm,fpm,fnm,tpm]]=load_confusion_matrices()\n",
    "    PE = tn/(fp+tn)\n",
    "    PEm = tnm/(fpm+tnm)\n",
    "    return [PEm, PE, PEm -PE]\n",
    "predictive_equality()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 5: Write Python function; group fairness taxonomy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have not done this use case becauses it is for the excellent criteria for the ILO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-case 6: Apply one/multiple explainable AI method(s) to the image classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your text for use-case 6 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf_explain in c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages (0.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages (from opencv-python) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\neilr\\documents\\anaconda\\envs\\blockc\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf_explain\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "\n",
    "#load GradCAM\n",
    "from tf_explain.core.grad_cam import GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = r\"C:\\Users\\neilr\\github-classroom\\BredaUniversityADSAI\\2022-23c-1fcmgt-reg-ai-01-neildaniel221270\\360_F_368360619_9ddfAPtaAp0ZMPdroyQIMIhhS6SyKCVK.jpeg\" \n",
    "class_index = 281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n",
    "img = tf.keras.preprocessing.image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.applications.vgg16.VGG16(weights=\"imagenet\", include_top=True)\n",
    "#get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first create the input in a format that the explainer expects (a tuple)\n",
    "input_img = (np.array([img]), None)\n",
    "\n",
    "#initialize the explainer as an instance of the GradCAM object\n",
    "explainer = GradCAM()\n",
    "\n",
    "# Obtain explanations for your image using VGG 16 and GradCAM\n",
    "grid = explainer.explain(input_img,\n",
    "                         model,\n",
    "                         class_index=class_index\n",
    "                         )\n",
    "\n",
    "#save the resulting image\n",
    "explainer.save(grid, \"C:/Users/neilr/github-classroom/BredaUniversityADSAI/2022-23c-1fcmgt-reg-ai-01-neildaniel221270/\", \"grad_cam_spoiled1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH2 = r\"C:\\Users\\neilr\\github-classroom\\BredaUniversityADSAI\\2022-23c-1fcmgt-reg-ai-01-neildaniel221270\\Picture1.jpg\" \n",
    "class_index = 281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = tf.keras.preprocessing.image.load_img(IMAGE_PATH2, target_size=(224, 224))\n",
    "img2 = tf.keras.preprocessing.image.img_to_array(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first create the input in a format that the explainer expects (a tuple)\n",
    "input_img = (np.array([img2]), None)\n",
    "\n",
    "#initialize the explainer as an instance of the GradCAM object\n",
    "explainer = GradCAM()\n",
    "\n",
    "# Obtain explanations for your image using VGG 16 and GradCAM\n",
    "grid = explainer.explain(input_img,\n",
    "                         model,\n",
    "                         class_index=class_index\n",
    "                         )\n",
    "\n",
    "#save the resulting image\n",
    "explainer.save(grid, \"C:/Users/neilr/github-classroom/BredaUniversityADSAI/2022-23c-1fcmgt-reg-ai-01-neildaniel221270/\", \"grad_cam_notspoiled1.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imsitu_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f1b8e97ac6f9e935fc32d1539b028242da6dac791077058bdcee137f54e5c9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
