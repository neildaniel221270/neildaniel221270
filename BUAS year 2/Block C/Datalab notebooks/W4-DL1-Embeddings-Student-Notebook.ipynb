{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "directed-spouse",
   "metadata": {},
   "source": [
    "Before starting the DataLab, preprocess the tweets like you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acknowledged-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bottom-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\neilr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naval-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "historic-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "better-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "optical-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neilr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dirty-sheet",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a list of token\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "    - Tokenizes\n",
    "    - Removes stopwords and punctuation\n",
    "    - Stem tokens\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE #\n",
    "    # Remove URLs using regular expression\n",
    "    processed_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    processed_tweet = re.sub(r'#([^\\s]+)', r'\\1', processed_tweet)\n",
    "    \n",
    "    # Tokenize the tweet using TweetTokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "    processed_tweet = tokenizer.tokenize(processed_tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_english = set(stopwords.words('english'))\n",
    "    processed_tweet = [word for word in processed_tweet if word not in stopwords_english]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    processed_tweet = [word for word in processed_tweet if word not in string.punctuation]\n",
    "    \n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_tweet = [stemmer.stem(word) for word in processed_tweet]\n",
    "\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "piano-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "tweet = all_positive_tweets[2277]\n",
    "tweet_processed = tweet_processor(tweet)\n",
    "\n",
    "print(tweet)\n",
    "print(tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "patient-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% training 20% testing\n",
    "positive_tweets_tr = all_positive_tweets[:4000]\n",
    "positive_tweets_te = all_positive_tweets[4000:]\n",
    "\n",
    "negative_tweets_tr = all_negative_tweets[:4000]\n",
    "negative_tweets_te = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "noted-genius",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tweet_processor_list(tweet_list):\n",
    "    # YOUR CODE HERE #\n",
    "    processed_tweet_list = []\n",
    "    for tweet in tweet_list:\n",
    "        processed_tweet = tweet_processor(tweet)\n",
    "        processed_tweet_list.append(processed_tweet)\n",
    "    return processed_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ruled-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets_tr = tweet_processor_list(positive_tweets_tr)\n",
    "positive_tweets_te = tweet_processor_list(positive_tweets_te)\n",
    "\n",
    "negative_tweets_tr = tweet_processor_list(negative_tweets_tr)\n",
    "negative_tweets_te = tweet_processor_list(negative_tweets_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-builder",
   "metadata": {},
   "source": [
    "You already did the steps until here in the previous DataLab. Let's do a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "passive-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(positive_tweets_tr) == 4000\n",
    "assert len(negative_tweets_tr) == 4000\n",
    "\n",
    "assert len(positive_tweets_te) == 1000\n",
    "assert len(negative_tweets_te) == 1000\n",
    "\n",
    "assert type(positive_tweets_tr) is list\n",
    "assert type(positive_tweets_tr[0]) is list\n",
    "assert type(positive_tweets_tr[0][0]) is str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-summary",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this DataLab you will train a neural network with an embedding layer for classifying tweets as positive and negative.\n",
    "\n",
    "Until now, we based our models (Logistic Regression and Naive Bayes) on counting the words. For the first time, we will try to capture meaning as well by using embeddings.\n",
    "\n",
    "To achieve this, we need to represent words as numbers because neural networks work with numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-entry",
   "metadata": {},
   "source": [
    "## 1. Converting words to numbers\n",
    "\n",
    "Before using the embedding layer, we need to convert tokens to numbers. The best way to explain how to do it is to actually show it. So let's use an example corpus with only 3 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "agreed-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents = ['This is a tasty apple.',\n",
    "                      'Hello John!',\n",
    "                      'I liked the movie.',\n",
    "                      'I have a car.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-nightlife",
   "metadata": {},
   "source": [
    "Let's use `Tokenizer` from Keras to convert these documents to numbers:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lasting-collapse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\neilr\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-style",
   "metadata": {},
   "source": [
    "What we just did was to simply tokenize all the documents, find the unique tokens (or words) and assign a number (index) to them. You can view the results by using the `word_index` attribute on the `tokenizer` object, which in fact is our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "collectible-discharge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'i': 2,\n",
       " 'this': 3,\n",
       " 'is': 4,\n",
       " 'tasty': 5,\n",
       " 'apple': 6,\n",
       " 'hello': 7,\n",
       " 'john': 8,\n",
       " 'liked': 9,\n",
       " 'the': 10,\n",
       " 'movie': 11,\n",
       " 'have': 12,\n",
       " 'car': 13}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-death",
   "metadata": {},
   "source": [
    "Now I can use the `tokenizer` object again to convert documents to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "delayed-norman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 1, 5, 6], [7, 8], [2, 9, 10, 11], [2, 12, 1, 13]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(training_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-george",
   "metadata": {},
   "source": [
    "`'Hello John!'` becomes `[7, 8]`.\n",
    "\n",
    "But what happens if you would like to convert a document with an unknown word (out of vocabulary)? For example `'Hello Mary!'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stupid-science",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Hello Mary!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-accreditation",
   "metadata": {},
   "source": [
    "Unknown word is ignored. This is not ideal. It is better to have a special token that indicates unknown words. Let's repeat the steps above but this time with `oov_token=\"<OOV>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "appreciated-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "loving-amino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'a': 2,\n",
       " 'i': 3,\n",
       " 'this': 4,\n",
       " 'is': 5,\n",
       " 'tasty': 6,\n",
       " 'apple': 7,\n",
       " 'hello': 8,\n",
       " 'john': 9,\n",
       " 'liked': 10,\n",
       " 'the': 11,\n",
       " 'movie': 12,\n",
       " 'have': 13,\n",
       " 'car': 14}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-morris",
   "metadata": {},
   "source": [
    "This time we have a new token for out of vocabulary words `<OOV>` with `1` as its index. Which means whenever we have an unknown word, it will be indexed as `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "wooden-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 1]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Hello Mary!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-emergency",
   "metadata": {},
   "source": [
    "**Task 1.1**\n",
    "\n",
    "Write a document with a few words and convert it to numbers using `tokenizer.texts_to_sequences()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "essential-taylor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "document = ['I love playing football with my friends!']\n",
    "\n",
    "# Convert it to numbers\n",
    "document_numbers = tokenizer.texts_to_sequences(document)\n",
    "document_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-collins",
   "metadata": {},
   "source": [
    "Let's take a look at our corpus turned into sequences again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "illegal-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 2, 6, 7], [8, 9], [3, 10, 11, 12], [3, 13, 2, 14]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(training_documents)\n",
    "training_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-correlation",
   "metadata": {},
   "source": [
    "Notice that naturally, the length of the sentences are different. But typically machine learning models expect a fixed input size, in other words a fixed number of features. Handling this is as easy as padding the short sentences with zeros. This can be done using `pad_sequences` from Keras.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "military-organizer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  2,  6,  7],\n",
       "       [ 0,  0,  0,  8,  9],\n",
       "       [ 0,  3, 10, 11, 12],\n",
       "       [ 0,  3, 13,  2, 14]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "pad_sequences(training_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-pontiac",
   "metadata": {},
   "source": [
    "Zeros are added to the short sentences and now all sequences are 5 numbers long. Now let's dig deeper to understand the default arguments and how to change them.\n",
    "\n",
    "For example by default zeros are added to the left. We can use `padding='post'` to add zeros to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "successful-panel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  2,  6,  7],\n",
       "       [ 8,  9,  0,  0,  0],\n",
       "       [ 3, 10, 11, 12,  0],\n",
       "       [ 3, 13,  2, 14,  0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(training_sequences,\n",
    "              padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-amazon",
   "metadata": {},
   "source": [
    "Note that the length of the sequences are equal to the longest sentence. We can make it shorter or longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "similar-greene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6,  7],\n",
       "       [ 8,  9,  0],\n",
       "       [10, 11, 12],\n",
       "       [13,  2, 14]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(training_sequences,\n",
    "              padding='post',\n",
    "              maxlen=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "color-engineer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  2,  6,  7,  0],\n",
       "       [ 8,  9,  0,  0,  0,  0],\n",
       "       [ 3, 10, 11, 12,  0,  0],\n",
       "       [ 3, 13,  2, 14,  0,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(training_sequences,\n",
    "              padding='post',\n",
    "              maxlen=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-amount",
   "metadata": {},
   "source": [
    "and as you might have guessed we can decide if we want to truncate from left or right of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "limiting-journey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  2],\n",
       "       [ 8,  9,  0],\n",
       "       [ 3, 10, 11],\n",
       "       [ 3, 13,  2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(training_sequences,\n",
    "              padding='post',\n",
    "              maxlen=3,\n",
    "              truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-blond",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "We started with a list of strings, in other words a corpus with documents.\n",
    "\n",
    "```\n",
    "training_documents = ['This is a tasty apple.',\n",
    "                      'Hello John!',\n",
    "                      'I liked the movie.',\n",
    "                      'I have a car.']\n",
    "```\n",
    "\n",
    "Then, fit a `tokenizer` to it, which assigned a number to every unique word.\n",
    "\n",
    "```\n",
    "{'<OOV>': 1,\n",
    " 'a': 2,\n",
    " 'i': 3,\n",
    " 'this': 4,\n",
    " 'is': 5,\n",
    " 'tasty': 6,\n",
    " 'apple': 7,\n",
    " 'hello': 8,\n",
    " 'john': 9,\n",
    " 'liked': 10,\n",
    " 'the': 11,\n",
    " 'movie': 12,\n",
    " 'have': 13,\n",
    " 'car': 14}\n",
    "```\n",
    "\n",
    "Finally we converted strings into numbers. We used padding to obtain a fixed length for sequences.\n",
    "\n",
    "```\n",
    "array([[ 4,  5,  2,  6,  7],\n",
    "       [ 8,  9,  0,  0,  0],\n",
    "       [ 3, 10, 11, 12,  0],\n",
    "       [ 3, 13,  2, 14,  0]], dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-republic",
   "metadata": {},
   "source": [
    "## 2. Converting tweets to numbers\n",
    "\n",
    "Now it is time to apply what you have learned to tweets. But let's first create `training_tweets` and `test_tweets` by combining positive and negative tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adopted-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets = positive_tweets_tr + negative_tweets_tr\n",
    "test_tweets = positive_tweets_te + negative_tweets_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-fundamentals",
   "metadata": {},
   "source": [
    "While we are creating our dataset, we can also create our labels. We know that first half of `training_tweets` and `test_tweets` are positive (label = 1) and second half is negative (label = 0). Therefore creating the labels is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "united-tokyo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.append(np.ones(len(positive_tweets_tr)),\n",
    "                    np.zeros(len(negative_tweets_tr)))\n",
    "\n",
    "y_test = np.append(np.ones(len(positive_tweets_te)),\n",
    "                   np.zeros(len(negative_tweets_te)))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-viewer",
   "metadata": {},
   "source": [
    "Remember that we already preprocessed and tokenized our tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "novel-bishop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['followfriday',\n",
       " '@france_int',\n",
       " '@pkuchly57',\n",
       " '@milipol_pari',\n",
       " 'top',\n",
       " 'engag',\n",
       " 'member',\n",
       " 'commun',\n",
       " 'week',\n",
       " ':)']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-yeast",
   "metadata": {},
   "source": [
    "But Keras `Tokenizer()` expects a list of strings. So let's combine tokens into strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lyric-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets_str = []\n",
    "for tw in training_tweets:\n",
    "    training_tweets_str.append(' '.join(tw))\n",
    "    \n",
    "test_tweets_str = []\n",
    "for tw in test_tweets:\n",
    "    test_tweets_str.append(' '.join(tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "czech-setting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'followfriday @france_int @pkuchly57 @milipol_pari top engag member commun week :)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-technician",
   "metadata": {},
   "source": [
    "**Task 2.1**\n",
    "\n",
    "Use tokenizer on `training_tweets_str`. Notice that tokenizer processes text with the `filters` parameter. Set it to `filters=''` to prevent processing because we already processed our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "nutritional-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(training_tweets_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-continuity",
   "metadata": {},
   "source": [
    "**Task 2.2**\n",
    "\n",
    "Calculate the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dirty-herald",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14885\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-brown",
   "metadata": {},
   "source": [
    "**Task 2.3**\n",
    "\n",
    "Find the numbers that represent the words `'boy'`, `'girl'`, `'man'` and `'woman'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eleven-insurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of 'boy' is 354\n",
      "The index of 'girl' is 154\n",
      "The index of 'man' is 203\n",
      "The index of 'woman' is 897\n"
     ]
    }
   ],
   "source": [
    "boy_index = tokenizer.word_index.get('boy', 0)\n",
    "girl_index = tokenizer.word_index.get('girl', 0)\n",
    "man_index = tokenizer.word_index.get('man', 0)\n",
    "woman_index = tokenizer.word_index.get('woman', 0)\n",
    "\n",
    "print(f\"The index of 'boy' is {boy_index}\")\n",
    "print(f\"The index of 'girl' is {girl_index}\")\n",
    "print(f\"The index of 'man' is {man_index}\")\n",
    "print(f\"The index of 'woman' is {woman_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-relay",
   "metadata": {},
   "source": [
    "**Task 2.4**\n",
    "\n",
    "Convert training and test tweets to sequences and use padding.\n",
    "\n",
    "Example tweet:\n",
    "\n",
    "`'followfriday top engag member commun week :)'`\n",
    "\n",
    "Corresponding sequence:\n",
    "\n",
    "`[347, 221, 937, 400, 286, 52, 3]`\n",
    "\n",
    "Padded sequence:\n",
    "\n",
    "`array([347, 221, 937, 400, 286,  52,   3,   0,   0,   0,   0,   0,   0,\n",
    "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "         0,   0,   0,   0], dtype=int32)`\n",
    "\n",
    "\n",
    "For padding arguments use `padding='post'` and `maxlen=30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "discrete-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(training_tweets_str)\n",
    "training_padded = pad_sequences(training_sequences, padding='post', maxlen=30)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_tweets_str)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fossil-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert training_padded.shape == (8000, 30)\n",
    "assert test_padded.shape == (2000, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-justice",
   "metadata": {},
   "source": [
    "## 3. Build a neural network with an embedding layer\n",
    "\n",
    "You just converted tweets to numbers, now we are ready to train a neural network on this dataset. Let's first define `X_train`, `y_train`, `X_test`, `y_test`. We already created the labels and padded sequences will be our `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "charged-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_padded\n",
    "X_test = test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "flush-defeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 30), (8000,), (2000, 30), (2000,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-madison",
   "metadata": {},
   "source": [
    "**Task 3.1**\n",
    "\n",
    "Build a `Sequential` model from Keras. First layer should be an `Embedding` layer. In the `Embedding` layer define the following parameters:\n",
    "\n",
    ">input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "\n",
    ">output_dim: Integer. Dimension of the dense embedding.\n",
    "\n",
    ">input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).\n",
    "\n",
    "Note that `input_dim` is `vocab_size + 1` because we are padding with zeros. For `output_dim` please use `2` because we would like to plot the embeddings. Finally for `input_length` use `30` because we used `maxlen=30` during padding.\n",
    "\n",
    "After the `Embedding` layer flatten its output and connect `Dense` layers. As a last layer, add a `Dense` layer suitable for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "subject-familiar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\neilr\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 2)             29772     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 60)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                3904      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33741 (131.80 KB)\n",
      "Trainable params: 33741 (131.80 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size + 1, output_dim=2, input_length=30))\n",
    "\n",
    "# Flatten the output of the Embedding layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add Dense layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-customer",
   "metadata": {},
   "source": [
    "**Task 3.2**\n",
    "\n",
    "Compile the model by selecting a proper loss, optimizer and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "indie-missouri",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\neilr\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-manhattan",
   "metadata": {},
   "source": [
    "**Task 3.3**\n",
    "\n",
    "Train the model with `X_train` and `y_train`. Use `X_test` and `y_test` as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adult-benefit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\neilr\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\neilr\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4451 - accuracy: 0.8508 - val_loss: 0.0793 - val_accuracy: 0.9950\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.9955 - val_loss: 0.0181 - val_accuracy: 0.9975\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 0.0119 - val_accuracy: 0.9975\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 0.0090 - val_accuracy: 0.9975\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0077 - val_accuracy: 0.9980\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 0.9975\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 6.6159e-04 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 0.9980\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 4.4780e-04 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9980\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 3.1448e-04 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 0.9975\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.2271e-04 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 0.9975\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-somalia",
   "metadata": {},
   "source": [
    "You just developed a neural network for sentiment analysis congrats!\n",
    "\n",
    "**Task 3.4**\n",
    "\n",
    "Predict the class of a test tweet.\n",
    "\n",
    "Example tweet:\n",
    "`\"back thnx god i'm happi :)\"`\n",
    "\n",
    "Model prediction:\n",
    "`array([[0.99999976]], dtype=float32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "international-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9999123]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "example_tweet = \"back thnx god i'm happy :)\"\n",
    "\n",
    "# Tokenize and pad the example tweet\n",
    "example_sequence = tokenizer.texts_to_sequences([example_tweet])\n",
    "example_padded = pad_sequences(example_sequence, padding='post', maxlen=30)\n",
    "\n",
    "# Predict the class\n",
    "prediction = model.predict(example_padded)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-amsterdam",
   "metadata": {},
   "source": [
    "## 4. Semantic properties of embeddings\n",
    "\n",
    "Take a look at Figures 6.15 and 6.16 (Section 6.10, page 126, Speech and Language Processing). They show one of the ways embeddings capture meaning in language. Let's try if embeddings from our model learned any semantic properties.\n",
    "\n",
    "In order to achieve this we need to access the output of our trained embedding layer, which is the first layer. We can access each layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "material-dragon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.embedding.Embedding at 0x26c993ccd50>,\n",
       " <keras.src.layers.reshaping.flatten.Flatten at 0x26c999b34d0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x26c94ef17d0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x26c8f412010>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-fifty",
   "metadata": {},
   "source": [
    "and input/output of a layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "applied-edmonton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 30) dtype=float32 (created by layer 'embedding_input')>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "australian-klein",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 30, 2) dtype=float32 (created by layer 'embedding')>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-buffer",
   "metadata": {},
   "source": [
    "We can create a new model only from the trained Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "korean-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "embedding_model = Model(inputs=model.layers[0].input,\n",
    "                        outputs=model.layers[0].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-tower",
   "metadata": {},
   "source": [
    "`embedding_model` accepts sequences as input and returns the output of the embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-french",
   "metadata": {},
   "source": [
    "**Task 4.1**\n",
    "\n",
    "We can use this model, just like any model. Use the `embedding_model` on the test tweet you used for Task 3.4 and get the embeddings. You should obtain two numbers for each word in the tweet. These two numbers are called word embeddings.\n",
    "\n",
    "Example tweet:\n",
    "\n",
    "`\"back thnx god i'm happi :)\"`\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "array([[[ 2.03354090e-01, -1.61128387e-01],\n",
    "        [ 3.02020200e-02, -4.17610519e-02],\n",
    "        [-6.28291816e-03, -3.17817833e-03],\n",
    "        [ 1.16536245e-01, -1.00184120e-01],\n",
    "        [-2.93731004e-01,  2.99061388e-01],\n",
    "        [-1.42090285e+00,  1.37957323e+00],\n",
    "        [-7.92113831e-04,  2.80260388e-02],\n",
    "        [-7.92113831e-04,  2.80260388e-02],\n",
    "        [-7.92113831e-04,  2.80260388e-02],\n",
    "        ...\n",
    "        [-7.92113831e-04,  2.80260388e-02]]], dtype=float32)\n",
    "```\n",
    "\n",
    "where `'back'` is `[ 2.03354090e-01, -1.61128387e-01]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "stuffed-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.02959852, -0.05010784],\n",
       "        [-0.01541641,  0.0008052 ],\n",
       "        [ 0.02914905, -0.0649142 ],\n",
       "        [-0.468154  ,  0.39904687],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489],\n",
       "        [-0.05754199, -0.11808489]]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "# Example tweet\n",
    "example_tweet = \"back thnx god i'm happy :)\"\n",
    "\n",
    "# Tokenize the example tweet\n",
    "example_sequence = tokenizer.texts_to_sequences([example_tweet])\n",
    "\n",
    "# Pad the tokenized sequence\n",
    "example_padded = pad_sequences(example_sequence, padding='post', maxlen=30)\n",
    "\n",
    "# Get the embeddings using the embedding_model\n",
    "embeddings = embedding_model.predict(example_padded)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-store",
   "metadata": {},
   "source": [
    "Using the idea above, let's create a function that can give us a vector (word embedding) for any given word.\n",
    "\n",
    "**Task 4.2**\n",
    "\n",
    "Write a function that:\n",
    "- Accepts a word as a string\n",
    "```\n",
    "'man'\n",
    "```\n",
    "- Converts it into a sequence using the `tokenizer` you fitted previously\n",
    "```\n",
    "[[199]]\n",
    "```\n",
    "- Pads it\n",
    "```\n",
    "array([[199,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0]], dtype=int32)\n",
    "```\n",
    "- Uses the `embedding_model` to get the embeddings\n",
    "```\n",
    "array([[[ 0.13211662,  0.11832968],\n",
    "        [-0.03486646, -0.00182698],\n",
    "        [-0.03486646, -0.00182698],\n",
    "        [-0.03486646, -0.00182698],\n",
    "        [-0.03486646, -0.00182698],\n",
    "        ...\n",
    "        [-0.03486646, -0.00182698]]], dtype=float32)\n",
    "```\n",
    "- And finally returns the embedding (vector) that corresponds to the word e.g. \n",
    "```\n",
    "array([0.13211662, 0.11832968], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "relevant-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vector(word):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: a string containing the word\n",
    "    Output:\n",
    "        vector: a numpy array containing the vector obtained \n",
    "                from the trained embedding layer\n",
    "    \"\"\"\n",
    "    # Tokenize the word\n",
    "    word_sequence = tokenizer.texts_to_sequences([word])\n",
    "    \n",
    "    # Pad the tokenized sequence\n",
    "    padded_sequence = pad_sequences(word_sequence, padding='post', maxlen=30)\n",
    "    \n",
    "    # Get the embeddings using the embedding_model\n",
    "    embeddings = embedding_model.predict(padded_sequence)\n",
    "    \n",
    "    # Return the embedding (vector) corresponding to the word\n",
    "    return embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "endless-console",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.02573082, -0.04593407], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = word_to_vector('man')\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aggressive-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(vector) == np.ndarray\n",
    "assert vector.shape == (2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-report",
   "metadata": {},
   "source": [
    "**Task 4.3**\n",
    "\n",
    "Plot the embeddings of the following words `['boy', 'girl', 'man', 'woman']` and check if your model captures the male-female relation.\n",
    "\n",
    "<img src=https://i.imgur.com/CUZuvxW.png width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "municipal-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_plotter(words):\n",
    "    # Get embeddings for each word\n",
    "    vectors = [word_to_vector(word) for word in words]\n",
    "\n",
    "    # Extract x and y coordinates from embeddings\n",
    "    x = [vector[0] for vector in vectors]\n",
    "    y = [vector[1] for vector in vectors]\n",
    "\n",
    "    # Plot the embeddings\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (x[i], y[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Word Embeddings')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "athletic-championship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_plotter(['boy', 'girl', 'man', 'woman'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
