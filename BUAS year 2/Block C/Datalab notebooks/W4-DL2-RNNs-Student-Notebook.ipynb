{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corrected-islam",
   "metadata": {},
   "source": [
    "## 1. Tweet preprocessing (previous datalab)\n",
    "\n",
    "Before starting the DataLab, preprocess the tweets like you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acknowledged-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bottom-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\neilr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naval-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "historic-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "better-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optical-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neilr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dirty-sheet",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a list of token\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "    - Tokenizes\n",
    "    - Removes stopwords and punctuation\n",
    "    - Stem tokens\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE #\n",
    "    # Remove URLs using regular expression\n",
    "    processed_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    processed_tweet = re.sub(r'#([^\\s]+)', r'\\1', processed_tweet)\n",
    "    \n",
    "    # Tokenize the tweet using TweetTokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "    processed_tweet = tokenizer.tokenize(processed_tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_english = set(stopwords.words('english'))\n",
    "    processed_tweet = [word for word in processed_tweet if word not in stopwords_english]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    processed_tweet = [word for word in processed_tweet if word not in string.punctuation]\n",
    "    \n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_tweet = [stemmer.stem(word) for word in processed_tweet]\n",
    "\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "piano-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "tweet = all_positive_tweets[2277]\n",
    "tweet_processed = tweet_processor(tweet)\n",
    "\n",
    "print(tweet)\n",
    "print(tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "patient-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% training 20% testing\n",
    "positive_tweets_tr = all_positive_tweets[:4000]\n",
    "positive_tweets_te = all_positive_tweets[4000:]\n",
    "\n",
    "negative_tweets_tr = all_negative_tweets[:4000]\n",
    "negative_tweets_te = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noted-genius",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tweet_processor_list(tweet_list):\n",
    "    # YOUR CODE HERE #\n",
    "    processed_tweet_list = []\n",
    "    for tweet in tweet_list:\n",
    "        processed_tweet = tweet_processor(tweet)\n",
    "        processed_tweet_list.append(processed_tweet)\n",
    "    return processed_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ruled-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets_tr = tweet_processor_list(positive_tweets_tr)\n",
    "positive_tweets_te = tweet_processor_list(positive_tweets_te)\n",
    "\n",
    "negative_tweets_tr = tweet_processor_list(negative_tweets_tr)\n",
    "negative_tweets_te = tweet_processor_list(negative_tweets_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-abortion",
   "metadata": {},
   "source": [
    "You already did the steps until here in the previous DataLab. Let's do a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hydraulic-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(positive_tweets_tr) == 4000\n",
    "assert len(negative_tweets_tr) == 4000\n",
    "\n",
    "assert len(positive_tweets_te) == 1000\n",
    "assert len(negative_tweets_te) == 1000\n",
    "\n",
    "assert type(positive_tweets_tr) is list\n",
    "assert type(positive_tweets_tr[0]) is list\n",
    "assert type(positive_tweets_tr[0][0]) is str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-bahrain",
   "metadata": {},
   "source": [
    "## 2. Converting tweets to numbers (previous datalab)\n",
    "\n",
    "Repeat your code from the last datalab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "offensive-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\neilr\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hindu-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets = positive_tweets_tr + negative_tweets_tr\n",
    "test_tweets = positive_tweets_te + negative_tweets_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-wonder",
   "metadata": {},
   "source": [
    "While we are creating our dataset, we can also create our labels. We know that first half of `training_tweets` and `test_tweets` are positive (label = 1) and second half is negative (label = 0). Therefore creating the labels is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "expired-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.append(np.ones(len(positive_tweets_tr)),\n",
    "                    np.zeros(len(negative_tweets_tr)))\n",
    "\n",
    "y_test = np.append(np.ones(len(positive_tweets_te)),\n",
    "                   np.zeros(len(negative_tweets_te)))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-fusion",
   "metadata": {},
   "source": [
    "Remember that we already preprocessed and tokenized our tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "forward-sarah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['followfriday',\n",
       " '@france_int',\n",
       " '@pkuchly57',\n",
       " '@milipol_pari',\n",
       " 'top',\n",
       " 'engag',\n",
       " 'member',\n",
       " 'commun',\n",
       " 'week',\n",
       " ':)']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-sudan",
   "metadata": {},
   "source": [
    "But Keras `Tokenizer()` expects a list of strings. So let's combine tokens into strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "occupied-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets_str = []\n",
    "for tw in training_tweets:\n",
    "    training_tweets_str.append(' '.join(tw))\n",
    "    \n",
    "test_tweets_str = []\n",
    "for tw in test_tweets:\n",
    "    test_tweets_str.append(' '.join(tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "explicit-scheme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'followfriday @france_int @pkuchly57 @milipol_pari top engag member commun week :)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-charger",
   "metadata": {},
   "source": [
    "**Task 2.1**\n",
    "\n",
    "Use tokenizer on `training_tweets_str`. Notice that tokenizer processes text with the `filters` parameter. Set it to `filters=''` to prevent processing because we already processed our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nutritional-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(filters='')\n",
    "\n",
    "# Fit the tokenizer on training_tweets_str\n",
    "tokenizer.fit_on_texts(training_tweets_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-advance",
   "metadata": {},
   "source": [
    "**Task 2.2**\n",
    "\n",
    "Calculate the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dirty-herald",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14886\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 for the padding token\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-scientist",
   "metadata": {},
   "source": [
    "**Task 2.3**\n",
    "\n",
    "Find the numbers that represent the words `'boy'`, `'girl'`, `'man'` and `'woman'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eleven-insurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'girl': 154, 'man': 203, 'boy': 354, 'woman': 897}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "word_to_index = tokenizer.word_index\n",
    "numbers_representations = {word: index for word, index in word_to_index.items() if word in ['boy', 'girl', 'man', 'woman']}\n",
    "numbers_representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-picture",
   "metadata": {},
   "source": [
    "**Task 2.4**\n",
    "\n",
    "Convert training and test tweets to sequences and use padding.\n",
    "\n",
    "Example tweet:\n",
    "\n",
    "`'followfriday top engag member commun week :)'`\n",
    "\n",
    "Corresponding sequence:\n",
    "\n",
    "`[347, 221, 937, 400, 286, 52, 3]`\n",
    "\n",
    "Padded sequence:\n",
    "\n",
    "`array([347, 221, 937, 400, 286,  52,   3,   0,   0,   0,   0,   0,   0,\n",
    "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "         0,   0,   0,   0], dtype=int32)`\n",
    "\n",
    "\n",
    "For padding arguments use `padding='post'` and `maxlen=30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "discrete-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "# Convert training tweets to sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(training_tweets_str)\n",
    "\n",
    "# Pad the sequences\n",
    "training_padded = pad_sequences(training_sequences, padding='post', maxlen=30)\n",
    "\n",
    "# Convert test tweets to sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_tweets_str)\n",
    "\n",
    "# Pad the sequences\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fleet-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert training_padded.shape == (8000, 30)\n",
    "assert test_padded.shape == (2000, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-crystal",
   "metadata": {},
   "source": [
    "## 3. Combine embedding layer with RNNs (this datalab)\n",
    "\n",
    "Create a model with the following layers:\n",
    "- Embedding layer\n",
    "- Recurrent layer\n",
    "- Dense layer\n",
    "\n",
    "This is the minimum architecture. You can modify it to increase the number of layers or add new layers such as Dropout.\n",
    "\n",
    "Keras provides a few recurrent layers:\n",
    "\n",
    "- LSTM layer\n",
    "- GRU layer\n",
    "- SimpleRNN layer\n",
    "- TimeDistributed layer\n",
    "- Bidirectional layer\n",
    "- ConvLSTM1D layer\n",
    "- ConvLSTM2D layer\n",
    "- ConvLSTM3D layer\n",
    "- Base RNN layer\n",
    "\n",
    "https://keras.io/api/layers/recurrent_layers/\n",
    "\n",
    "For your recurrent layer try LSTM, GRU and SimpleRNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "elect-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_padded\n",
    "X_test = test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dietary-apollo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 30), (8000,), (2000, 30), (2000,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "intimate-premium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 30, 100)           1488600   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1605977 (6.13 MB)\n",
      "Trainable params: 1605977 (6.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, GRU, SimpleRNN, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "max_length = 30\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-overall",
   "metadata": {},
   "source": [
    "**Task 3.2**\n",
    "\n",
    "Compile the model by selecting a proper loss, optimizer and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "important-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-lesbian",
   "metadata": {},
   "source": [
    "**Task 3.3**\n",
    "\n",
    "Train the model with `X_train` and `y_train`. Use `X_test` and `y_test` as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adult-benefit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 4s 19ms/step - loss: 0.2457 - accuracy: 0.8631 - val_loss: 0.0215 - val_accuracy: 0.9950\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.0093 - accuracy: 0.9975 - val_loss: 0.0310 - val_accuracy: 0.9935\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 3.1086e-04 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9930\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0956 - val_accuracy: 0.9830\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 1.8645e-04 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 0.9840\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-metro",
   "metadata": {},
   "source": [
    "**Task 3.4**\n",
    "\n",
    "Predict the class of a test tweet.\n",
    "\n",
    "Example tweet:\n",
    "`\"back thnx god i'm happi :)\"`\n",
    "\n",
    "Model prediction:\n",
    "`array([[0.99999976]], dtype=float32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "foreign-hazard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@heyclairee back thnx god i'm happi :)\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "familiar-applicant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 358ms/step\n",
      "Model prediction: [[0.9998839]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE #\n",
    "example_tweet = test_tweets_str[1]\n",
    "# Tokenize and pad the example tweet\n",
    "example_sequence = tokenizer.texts_to_sequences([example_tweet])\n",
    "example_padded = pad_sequences(example_sequence, padding='post', maxlen=30)\n",
    "\n",
    "# Predict the class of the example tweet\n",
    "prediction = model.predict(example_padded)\n",
    "\n",
    "print(\"Model prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-credit",
   "metadata": {},
   "source": [
    "## 4. Bidirectional LSTM (this datalab)\n",
    "\n",
    "Try bidirectional LSTMs.\n",
    "\n",
    "https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
