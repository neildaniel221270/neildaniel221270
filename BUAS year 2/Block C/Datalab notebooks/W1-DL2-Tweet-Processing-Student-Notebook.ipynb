{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "formed-utilization",
   "metadata": {},
   "source": [
    "Next week, you will start developing NLP models to classify positive and negative tweets. You will use the NLTK twitter samples dataset.\n",
    "\n",
    "**Twitter Samples dataset documentation**\n",
    "\n",
    "These samples of Tweets (or 'status updates') were collected from the\n",
    "Twitter Streaming and REST APIs. Each file consists of\n",
    "line-separated JSON-formatted tweets, i.e. one Tweet per line. For a\n",
    "detailed description of the JSON fields in a Tweet, see\n",
    "https://dev.twitter.com/overview/api/tweets.\n",
    "\n",
    "Any use of this data is subject to the Twitter Developer Agreement and\n",
    "Developer Policy:\n",
    "https://dev.twitter.com/overview/terms/agreement-and-policy.\n",
    "\n",
    "These were collected in July 2015 by searching against the following strings:\n",
    "\n",
    "positive_tweets.json\n",
    "\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    }\n",
    "\n",
    "negative_tweets.json\n",
    "\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-crest",
   "metadata": {},
   "source": [
    "Before developing sentiment analysis models on this dataset, you need to be able to process tweets. This is what you will learn in this datalab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acknowledged-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bottom-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\neilr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naval-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-norman",
   "metadata": {},
   "source": [
    "**Task 1: Start with a simple EDA on the dataset.**\n",
    "\n",
    "- What is the data type of `all_positive_tweets` and `all_negative_tweets`\n",
    "- What is the data type of a tweet entry\n",
    "- How many tweets are there in each class\n",
    "- Print a single tweet from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "historic-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n",
      "\n",
      "The type of all_positive_tweets is:  <class 'list'>\n",
      "The type of a tweet entry is:  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "undefined-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive class:\n",
      "I am not looking forward to a 12 hour shift today :))))))\n",
      "\n",
      "Negative class:\n",
      "@koolaidkiller18 no idea why :(\n"
     ]
    }
   ],
   "source": [
    "print('Positive class:')\n",
    "print(all_positive_tweets[random.randint(0,5000)]+'\\n')\n",
    "\n",
    "print('Negative class:')\n",
    "print(all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "anonymous-enlargement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n"
     ]
    }
   ],
   "source": [
    "example_tweet = ('My beautiful sunflowers on a sunny Friday morning off :)'\n",
    "                 ' #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i')\n",
    "print(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-conviction",
   "metadata": {},
   "source": [
    "Next, you will create a function called `tweet_processor()`. Task by task, you will add more functionality to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "matched-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a string containing the processed tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_tweet = tweet # no processing so far\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-vegetable",
   "metadata": {},
   "source": [
    "**Task 2: Remove hyperlinks from a tweet**\n",
    "\n",
    "Using `re.sub()`, write a regular expression to remove hyperlinks. Add this to the function `tweet_processor()`.\n",
    "\n",
    "Example tweet:\n",
    "`My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i`\n",
    "\n",
    "Expected output:\n",
    "`'My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… '`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "apart-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "plastic-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a string containing the processed tweet\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove URLs using regular expression\n",
    "    processed_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cardiac-storage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_processor(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-frame",
   "metadata": {},
   "source": [
    "**Task 3: Remove hashtags from the tweets**\n",
    "\n",
    "Add a `re.sub()` and a suitable pattern to the `tweet_processor()` function to remove the `#` sign from the word.\n",
    "\n",
    "Example tweet:\n",
    "`My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i`\n",
    "\n",
    "Expected output:\n",
    "`'My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… '`\n",
    "\n",
    "Notice that `tweet_processor()` now removes both hyperlinks and hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "useful-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a string containing the processed tweet\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove URLs using regular expression\n",
    "    processed_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    processed_tweet = re.sub(r'#([^\\s]+)', r'\\1', processed_tweet)\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "level-repeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_processor(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-wildlife",
   "metadata": {},
   "source": [
    "**Task 4: Tokenize a tweet**\n",
    "\n",
    "Use `TweetTokenizer` from `nltk` to tokenize tweets. Add this to the `tweet_processor()` function.\n",
    "\n",
    "Example tweet:\n",
    "`My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i`\n",
    "\n",
    "Expected output:\n",
    "`['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']`\n",
    "\n",
    "Notice that the output is not a string anymore. It is a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stunning-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-counter",
   "metadata": {},
   "source": [
    "Decide which arguments to provide to the following parameters:\n",
    "\n",
    "- **preserve_case** (bool) – Flag indicating whether to preserve the casing (capitalisation) of text used in the tokenize method. Defaults to True. \n",
    "\n",
    "- **reduce_len** (bool) – Flag indicating whether to replace repeated character sequences of length 3 or greater with sequences of length 3. Defaults to False.\n",
    "\n",
    "- **strip_handles** (bool) – Flag indicating whether to remove Twitter handles of text used in the tokenize method. Defaults to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cultural-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a list of tokens\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "    - Tokenizes\n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove URLs using regular expression\n",
    "    processed_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    processed_tweet = re.sub(r'#([^\\s]+)', r'\\1', processed_tweet)\n",
    "    \n",
    "    # Tokenize the tweet using TweetTokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "    processed_tweet = tokenizer.tokenize(processed_tweet)\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "honest-monthly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_processor(example_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-minutes",
   "metadata": {},
   "source": [
    " **Task 5: Remove stopwords and punctuation.**\n",
    " \n",
    " Go through every word in your tokens list, remove stopwords and punctuation. Add this to the function `tweet_processor()`.\n",
    " \n",
    " Example tweet:\n",
    "`My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i`\n",
    "\n",
    "Expected output:\n",
    "`['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']`\n",
    "\n",
    "Take a look at which tokens are removed and try to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "recorded-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neilr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_english = stopwords.words('english') \n",
    "print(stopwords_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "listed-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "powered-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a list of token\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "    - Tokenizes\n",
    "    - Removes stopwords and punctuation\n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove URLs using regular expression\n",
    "    processed_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    processed_tweet = re.sub(r'#([^\\s]+)', r'\\1', processed_tweet)\n",
    "    \n",
    "    # Tokenize the tweet using TweetTokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "    processed_tweet = tokenizer.tokenize(processed_tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_english = set(stopwords.words('english'))\n",
    "    processed_tweet = [word for word in processed_tweet if word not in stopwords_english]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    processed_tweet = [word for word in processed_tweet if word not in string.punctuation]\n",
    "    \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "functioning-professional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_processor(example_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-feedback",
   "metadata": {},
   "source": [
    "**Task 6: Stem the tokens**\n",
    "\n",
    "Use PorterStterm from NLTK to convert tokens to stems.\n",
    "https://en.wikipedia.org/wiki/Stemming\n",
    "\n",
    "Add this functionality to tweet_processor() function.\n",
    "\n",
    "Example tweet:\n",
    "`My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i`\n",
    "\n",
    "Expected output:\n",
    "`['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "received-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "everyday-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a list of token\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "    - Tokenizes\n",
    "    - Removes stopwords and punctuation\n",
    "    - Stem tokens\n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove URLs using regular expression\n",
    "    processed_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    processed_tweet = re.sub(r'#([^\\s]+)', r'\\1', processed_tweet)\n",
    "    \n",
    "    # Tokenize the tweet using TweetTokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "    processed_tweet = tokenizer.tokenize(processed_tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_english = set(stopwords.words('english'))\n",
    "    processed_tweet = [word for word in processed_tweet if word not in stopwords_english]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    processed_tweet = [word for word in processed_tweet if word not in string.punctuation]\n",
    "    \n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_tweet = [stemmer.stem(word) for word in processed_tweet]\n",
    "\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "buried-republican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_processor(example_tweet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
